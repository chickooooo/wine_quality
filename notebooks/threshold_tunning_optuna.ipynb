{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgbm\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing training data\n",
    "train = pd.read_csv('../data/train.csv').drop('Id',axis=1)\n",
    "\n",
    "# target and feature names\n",
    "target = 'quality'\n",
    "features = [c for c in train.columns if c not in ['id','Time', target]]\n",
    "\n",
    "# importing red wine original dataset\n",
    "orig_train = pd.read_csv('../red_white_dataset/winequality-red.csv', delimiter=';')\n",
    "# removing duplicates\n",
    "orig_train = orig_train[~orig_train.duplicated()]\n",
    "\n",
    "# concatting original data with training data\n",
    "train = pd.concat([train, orig_train]).reset_index(drop=True)\n",
    "# adding new column that labels datapoint as train or test\n",
    "train['split']= 'train'\n",
    "\n",
    "# importing test data\n",
    "test = pd.read_csv('../data/test.csv').drop('Id',axis=1)\n",
    "# labelling as test\n",
    "test['split'] = 'test'\n",
    "\n",
    "# combining train and test datasets\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# importing sample submission file\n",
    "sub = pd.read_csv('../data/submissions/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.073</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.99572</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.77</td>\n",
       "      <td>12.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>30.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99854</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.67</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.059</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            8.0              0.50         0.39             2.2      0.073   \n",
       "1            9.3              0.30         0.73             2.3      0.092   \n",
       "2            7.1              0.51         0.03             2.1      0.059   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 30.0                  39.0  0.99572  3.33       0.77   \n",
       "1                 30.0                  67.0  0.99854  3.32       0.67   \n",
       "2                  3.0                  12.0  0.99660  3.52       0.73   \n",
       "\n",
       "   alcohol  quality  split  \n",
       "0     12.1      6.0  train  \n",
       "1     12.8      6.0  train  \n",
       "2     11.3      7.0  train  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting total data\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3415, 13)\n",
      "(1372, 12)\n"
     ]
    }
   ],
   "source": [
    "# shape of train and test data\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtracting 3 from target so it's value ranges from 0 - 5\n",
    "train[target] = train[target] - 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # parameters searching space\n",
    "    params_optuna = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0001, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0001, 1.0),\n",
    "            'n_estimators': 10000,\n",
    "            'objective' : \"multi:softmax\",\n",
    "            'num_class': 5,\n",
    "    }\n",
    "\n",
    "    # cv fold setup\n",
    "    n = 10\n",
    "    cv = StratifiedKFold(n, shuffle=True, random_state=42)\n",
    "\n",
    "    # will hold scores of vaslidation set\n",
    "    fold_scores = []\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(cv.split(train[features], train[target])):\n",
    "\n",
    "        # splitting into training and validations set\n",
    "        X_train, y_train = train.loc[train_idx, features],train.loc[train_idx, target]\n",
    "        X_val, y_val = train.loc[val_idx, features],train.loc[val_idx, target]\n",
    "\n",
    "        # training model with train set\n",
    "        model = xgb.XGBClassifier(**params_optuna)\n",
    "        model.fit(X_train,\n",
    "                 y_train,\n",
    "                 eval_set= [(X_val, y_val)],\n",
    "                 early_stopping_rounds = 200,\n",
    "                 verbose=200)\n",
    "\n",
    "        # prediction score on validation set\n",
    "        pred_val = model.predict(X_val)\n",
    "        score = cohen_kappa_score(y_val, pred_val, weights='quadratic')\n",
    "\n",
    "        # appending score\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "    # return mean validation score\n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optuna study\n",
    "study = optuna.create_study(direction='maximize', sampler = TPESampler())\n",
    "\n",
    "# optimise the study\n",
    "study.optimize(func=objective, n_trials=1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best set of parameters\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best achieved value\n",
    "study.best_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # parameters searching space\n",
    "    params_optuna = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "            'n_estimators': 10000,\n",
    "            'objective' : \"multiclass\",\n",
    "            'metric' :'multi_logloss',\n",
    "    }\n",
    "\n",
    "    # cv fold setup\n",
    "    n = 10\n",
    "    cv = StratifiedKFold(n, shuffle=True, random_state=42)\n",
    "\n",
    "    # will hold scores of vaslidation set\n",
    "    fold_scores = []\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(cv.split(train[features], train[target])):\n",
    "\n",
    "        # splitting into training and validations set\n",
    "        X_train, y_train = train.loc[train_idx, features],train.loc[train_idx, target]\n",
    "        X_val, y_val = train.loc[val_idx, features],train.loc[val_idx, target]\n",
    "\n",
    "        # training model with train set\n",
    "        model = lgbm.LGBMClassifier(**params_optuna)\n",
    "        model.fit(X_train,\n",
    "                 y_train,\n",
    "                 eval_set= [(X_val, y_val)],\n",
    "                 early_stopping_rounds = 200,\n",
    "                 verbose=200)\n",
    "\n",
    "        # prediction score on validation set\n",
    "        pred_val = model.predict(X_val)\n",
    "        score = cohen_kappa_score(y_val,pred_val, weights='quadratic')\n",
    "\n",
    "        # appending score\n",
    "        fold_scores.append(score)\n",
    "    \n",
    "    # return mean validation score\n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler = TPESampler())\n",
    "study.optimize(func=objective, timeout=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best set of parameters\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best achieved value\n",
    "study.best_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xg_boost parameters set\n",
    "\n",
    "xgb_params_1 = {\n",
    "    'max_depth': 1,\n",
    "    'learning_rate': 0.38948972099234563,\n",
    "    'gamma': 0.5992826807539955,\n",
    "    'subsample': 0.760188310145391,\n",
    "    'colsample_bytree': 0.9470322372755515,\n",
    "    'reg_alpha': 0.8286639480742322,\n",
    "    'reg_lambda': 0.6987138355697013,\n",
    "    'n_estimators': 10000,\n",
    "    'num_class': 5,\n",
    "    'objective' : \"multi:softmax\",\n",
    "    'metric': 'multiclass',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'use_label_encoder': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light_gbm parameters set\n",
    "\n",
    "lgbm_params_1 = {\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': 8,\n",
    "    'lambda_l1': 1.9528554374745727e-09,\n",
    "    'lambda_l2': 6.288470302197343,\n",
    "    'feature_fraction': 0.8253409987746099,\n",
    "    'bagging_fraction': 0.6280124722436471,\n",
    "    'bagging_freq': 4,\n",
    "    'min_child_samples': 85,\n",
    "    'min_data_in_leaf': 93,\n",
    "    'n_estimators':10000,\n",
    "    'objective' : \"multiclass\",\n",
    "    'metric' :'multi_logloss'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:40:28] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.49383\n",
      "[200]\tvalidation_0-mlogloss:0.94957\n",
      "[262]\tvalidation_0-mlogloss:0.95374\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.02068\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's multi_logloss: 0.966673\n",
      "\n",
      "=== Fold 0 Cohen Kappa Score 0.3815267934061126 ===\n",
      "\n",
      "[06:40:32] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.49688\n",
      "[200]\tvalidation_0-mlogloss:0.98838\n",
      "[274]\tvalidation_0-mlogloss:0.99368\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.02096\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's multi_logloss: 0.984299\n",
      "\n",
      "=== Fold 1 Cohen Kappa Score 0.34402782760816636 ===\n",
      "\n",
      "[06:40:34] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.49394\n",
      "[200]\tvalidation_0-mlogloss:1.04437\n",
      "[275]\tvalidation_0-mlogloss:1.04460\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.12617\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's multi_logloss: 1.02446\n",
      "\n",
      "=== Fold 2 Cohen Kappa Score 0.35865791657024915 ===\n",
      "\n",
      "[06:40:36] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51151\n",
      "[200]\tvalidation_0-mlogloss:1.00806\n",
      "[249]\tvalidation_0-mlogloss:1.00340\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.07815\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's multi_logloss: 1.00168\n",
      "\n",
      "=== Fold 3 Cohen Kappa Score 0.3425000525246994 ===\n",
      "\n",
      "[06:40:38] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.49179\n",
      "[200]\tvalidation_0-mlogloss:0.96816\n",
      "[257]\tvalidation_0-mlogloss:0.97988\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.06868\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's multi_logloss: 0.970194\n",
      "\n",
      "=== Fold 4 Cohen Kappa Score 0.41278572322906104 ===\n",
      "\n",
      "[06:40:40] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51326\n",
      "[200]\tvalidation_0-mlogloss:1.01524\n",
      "[249]\tvalidation_0-mlogloss:1.01717\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.03093\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_logloss: 0.967624\n",
      "\n",
      "=== Fold 5 Cohen Kappa Score 0.3569988634561145 ===\n",
      "\n",
      "[06:40:42] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.53095\n",
      "[200]\tvalidation_0-mlogloss:1.01039\n",
      "[260]\tvalidation_0-mlogloss:1.01186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.06653\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's multi_logloss: 1.01991\n",
      "\n",
      "=== Fold 6 Cohen Kappa Score 0.34160528728606354 ===\n",
      "\n",
      "[06:40:44] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.52167\n",
      "[200]\tvalidation_0-mlogloss:0.96910\n",
      "[308]\tvalidation_0-mlogloss:0.98916\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.05945\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_logloss: 0.997968\n",
      "\n",
      "=== Fold 7 Cohen Kappa Score 0.35468647012788446 ===\n",
      "\n",
      "[06:40:46] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51525\n",
      "[200]\tvalidation_0-mlogloss:0.98956\n",
      "[331]\tvalidation_0-mlogloss:0.99676\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.06085\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's multi_logloss: 1.00006\n",
      "\n",
      "=== Fold 8 Cohen Kappa Score 0.32713112224897894 ===\n",
      "\n",
      "[06:40:48] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.52341\n",
      "[200]\tvalidation_0-mlogloss:1.03205\n",
      "[328]\tvalidation_0-mlogloss:1.04161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.10647\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's multi_logloss: 1.02506\n",
      "\n",
      "=== Fold 9 Cohen Kappa Score 0.3086535185221456 ===\n",
      "\n",
      "\n",
      "=== Average Cohen Kappa Score 0.3822206223749788 ===\n",
      "\n",
      "[06:40:50] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.49882\n",
      "[200]\tvalidation_0-mlogloss:1.01165\n",
      "[241]\tvalidation_0-mlogloss:1.01759\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.06706\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's multi_logloss: 0.988726\n",
      "\n",
      "=== Fold 0 Cohen Kappa Score 0.3717128828804238 ===\n",
      "\n",
      "[06:40:52] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.50452\n",
      "[200]\tvalidation_0-mlogloss:1.00262\n",
      "[260]\tvalidation_0-mlogloss:1.00863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.14846\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's multi_logloss: 1.00617\n",
      "\n",
      "=== Fold 1 Cohen Kappa Score 0.331399642661164 ===\n",
      "\n",
      "[06:40:54] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.50096\n",
      "[200]\tvalidation_0-mlogloss:1.05431\n",
      "[227]\tvalidation_0-mlogloss:1.05897\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.13969\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's multi_logloss: 1.02992\n",
      "\n",
      "=== Fold 2 Cohen Kappa Score 0.32661269523438496 ===\n",
      "\n",
      "[06:40:56] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.47399\n",
      "[200]\tvalidation_0-mlogloss:0.93946\n",
      "[327]\tvalidation_0-mlogloss:0.94917\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.00189\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's multi_logloss: 0.946338\n",
      "\n",
      "=== Fold 3 Cohen Kappa Score 0.38077474606498035 ===\n",
      "\n",
      "[06:40:58] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.50585\n",
      "[200]\tvalidation_0-mlogloss:1.00988\n",
      "[399]\tvalidation_0-mlogloss:1.02475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.09226\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's multi_logloss: 1.02123\n",
      "\n",
      "=== Fold 4 Cohen Kappa Score 0.33995592845306755 ===\n",
      "\n",
      "[06:41:00] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51613\n",
      "[200]\tvalidation_0-mlogloss:0.97440\n",
      "[299]\tvalidation_0-mlogloss:0.98363\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.03506\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's multi_logloss: 0.972468\n",
      "\n",
      "=== Fold 5 Cohen Kappa Score 0.32946041342175036 ===\n",
      "\n",
      "[06:41:03] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51955\n",
      "[200]\tvalidation_0-mlogloss:1.00060\n",
      "[381]\tvalidation_0-mlogloss:1.01136\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.08282\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's multi_logloss: 1.02386\n",
      "\n",
      "=== Fold 6 Cohen Kappa Score 0.32648468314651086 ===\n",
      "\n",
      "[06:41:05] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51174\n",
      "[200]\tvalidation_0-mlogloss:0.98181\n",
      "[369]\tvalidation_0-mlogloss:1.00292\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.06468\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's multi_logloss: 0.981924\n",
      "\n",
      "=== Fold 7 Cohen Kappa Score 0.3544487751678335 ===\n",
      "\n",
      "[06:41:07] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.52134\n",
      "[200]\tvalidation_0-mlogloss:0.98944\n",
      "[281]\tvalidation_0-mlogloss:0.99231\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.04754\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's multi_logloss: 0.984774\n",
      "\n",
      "=== Fold 8 Cohen Kappa Score 0.35376645873953905 ===\n",
      "\n",
      "[06:41:09] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"metric\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.51161\n",
      "[200]\tvalidation_0-mlogloss:0.99658\n",
      "[267]\tvalidation_0-mlogloss:0.99467\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8253409987746099, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8253409987746099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=93, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=93\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9528554374745727e-09, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9528554374745727e-09\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6280124722436471, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6280124722436471\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.288470302197343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.288470302197343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 1.05183\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_logloss: 0.993117\n",
      "\n",
      "=== Fold 9 Cohen Kappa Score 0.3890036603019468 ===\n",
      "\n",
      "\n",
      "=== Average Cohen Kappa Score 0.3790754211261336 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cv fold setup\n",
    "k = 10\n",
    "cv = StratifiedKFold(k, shuffle=True, random_state=z)\n",
    "\n",
    "# will hold scores of validation set\n",
    "fold_scores = []\n",
    "# will hold predictions of test set\n",
    "test_preds = []\n",
    "\n",
    "# will hold out of fold predictions\n",
    "oof_preds = []\n",
    "# will hold out of fold true values\n",
    "oof_true = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(cv.split(train[features], train[target])):\n",
    "    \n",
    "    # creating training and valisation data\n",
    "    X_train = train.loc[train_idx, features]\n",
    "    y_train = train.loc[train_idx, target]\n",
    "    X_val = train.loc[val_idx, features]\n",
    "    y_val = train.loc[val_idx, target]\n",
    "    \n",
    "\n",
    "\n",
    "    #************** XGBoost **************\n",
    "\n",
    "    # training xg_boost on training data\n",
    "    # with best set of parameters\n",
    "    model1 = xgb.XGBClassifier(**xgb_params_1)\n",
    "    model1.fit(X_train,\n",
    "            y_train,\n",
    "            eval_set= [(X_val,y_val)],\n",
    "            early_stopping_rounds = 200,\n",
    "            verbose=200)\n",
    "    \n",
    "    # prediction score on validation set\n",
    "    pred_val1 = model1.predict(X_val)\n",
    "    score1 = cohen_kappa_score(y_val, pred_val1)\n",
    "    \n",
    "    # discard the predictions of poor performing models\n",
    "    if score1 > 0.36:\n",
    "\n",
    "        # making predictions on test data\n",
    "        # and appending them to test_preds\n",
    "        test_preds.append(model1.predict(test[features]))\n",
    "\n",
    "        # appending validation score\n",
    "        fold_scores.append(score1)\n",
    "    \n",
    "\n",
    "\n",
    "    #************** LightGBM **************\n",
    "    \n",
    "    # training light_gbm on training data\n",
    "    # with best set of parameters\n",
    "    model2 = lgbm.LGBMClassifier(**lgbm_params_1)\n",
    "    model2.fit(X_train,\n",
    "            y_train,\n",
    "            eval_set= [(X_val,y_val)],\n",
    "            early_stopping_rounds = 200,\n",
    "            verbose=200)\n",
    "    \n",
    "    # prediction score on validation set\n",
    "    pred_val2 = model2.predict(X_val)\n",
    "    score2 = cohen_kappa_score(y_val, pred_val2)\n",
    "    \n",
    "    # discard the predictions of poor performing models\n",
    "    if score2 > 0.36:\n",
    "\n",
    "        # making predictions on test data\n",
    "        # and appending them to test_preds\n",
    "        test_preds.append(model2.predict(test[features]))\n",
    "\n",
    "        # appending validation score\n",
    "        fold_scores.append(score2)\n",
    "    \n",
    "\n",
    "    # appending mean of predictions from both models\n",
    "    oof_preds.extend(np.mean([pred_val1, pred_val2], axis=0))\n",
    "\n",
    "    # appending true values\n",
    "    oof_true.extend(y_val)\n",
    "\n",
    "    # printing average validation score for each fold\n",
    "    print('')\n",
    "    print(f'=== Fold {i} Cohen Kappa Score {np.mean([score1, score2])} ===')\n",
    "    print('')\n",
    "\n",
    "# printing total average validation score\n",
    "print('')\n",
    "print(f'=== Average Cohen Kappa Score {np.mean(fold_scores)} ===')\n",
    "print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Threshold Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaRounder:\n",
    "\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.labels = np.unique(y_true)\n",
    "\n",
    "    # getting best threshold values\n",
    "    def __call__(self, trial):\n",
    "        thresholds = []\n",
    "        for i in range(len(self.labels) - 1):\n",
    "            low = max(thresholds) if i > 0 else min(self.labels)\n",
    "            high = max(self.labels)\n",
    "            t = trial.suggest_float(f't{i}', low, high)\n",
    "            thresholds.append(t)\n",
    "        try:\n",
    "            opt_y_pred = self.adjust(self.y_pred, thresholds)\n",
    "        except: return 0\n",
    "        return cohen_kappa_score(self.y_true, opt_y_pred, weights='quadratic')\n",
    "\n",
    "    # modifying predictions as per obtained threshold\n",
    "    def adjust(self, y_pred, thresholds):\n",
    "        opt_y_pred = pd.cut(y_pred,\n",
    "                            [-np.inf] + thresholds + [np.inf],\n",
    "                            labels=self.labels)\n",
    "        return opt_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating objective\n",
    "objective = OptunaRounder(oof_true, oof_preds)\n",
    "# creating a study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "# optimising the study\n",
    "study.optimize(objective, timeout=60, n_jobs=-1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized thresholds: [1.430557892571371, 1.6233044347046632, 2.7367389713538812, 3.044667702903008, 3.885507912888741]\n",
      "Optimized OOF Score: 0.52871\n"
     ]
    }
   ],
   "source": [
    "# getting best thresholds\n",
    "best_thresholds = sorted(study.best_params.values())\n",
    "print(f'Optimized thresholds: {best_thresholds}')\n",
    "\n",
    "# modifying oof predictions\n",
    "oof_pred_opt = objective.adjust(oof_preds, best_thresholds)\n",
    "# getting score\n",
    "best_score = cohen_kappa_score(oof_true, oof_pred_opt, weights='quadratic')\n",
    "print(f'Optimized OOF Score: {best_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions\n",
    "test_preds = np.array(test_preds).mean(axis=0)\n",
    "\n",
    "# modifying test predictions\n",
    "# and adding 3\n",
    "opt_test_preds = objective.adjust(test_preds, best_thresholds).astype(int) +3\n",
    "\n",
    "# updating sample submission\n",
    "sub[target] = opt_test_preds\n",
    "# saving as a csv file\n",
    "sub.to_csv('../data/submission_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e9e12bc012fae32ac9654bb37cb31a0d4c91164effa766e52fa90ec4ccf2e77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
